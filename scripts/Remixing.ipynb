{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib widget\n",
    "\n",
    "import subprocess as sp\n",
    "\n",
    "from onset_detection import *\n",
    "from onset_detection.audio import *\n",
    "from onset_detection.od import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remixing\n",
    "This jupyter notebook allows the remixing of manually tapped rhythms back into its original track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick a song\n",
    "Choose a song you like from the list of songs below and listen back to it if you want:\n",
    "\n",
    "- The Beatles - In My Life: dict key = `\"beatles\"`\n",
    "- Queen - Another One Bites the Dust: dict key = `\"queen\"`\n",
    "- Arctic Monkeys - The View From the Afternoon: dict key = `\"arctic_monkeys\"`\n",
    "- Queens of the Stone Age - No One Knows: dict key = `\"qotsa\"`\n",
    "- Slow J - Mundança: dict key = `\"slowj\"`\n",
    "- AC/DC - Back in Black: dict key = `\"ac_dc\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "song = \"qotsa\"\n",
    "trim = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_path = song_dict[song]\n",
    "trim_interval = None\n",
    "# choose the interval in which you want to trim the audio (in seconds)\n",
    "if trim:\n",
    "    trim_interval = (70, 95)\n",
    "\n",
    "song_audio, song_sr = read_audio(song_path, trim_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(song_audio, rate=song_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Separating the drums from the rest\n",
    "\n",
    "Using S. Rouard et al.'s [Demucs](https://github.com/facebookresearch/demucs) [2], we can separate the drums from the rest of the other intruments:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_dir = \"../results/demucs\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "separate_drums = sp.run([\"python3.9\", \"-m\", \"demucs\", \"--two-stems=drums\", song_path, \"-o\", output_dir])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Listen back to the isolated drums, or to the original track without them"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drums only\n",
    "song_name = song_path.split('/')[-1].split('.')[0]\n",
    "drums_only = output_dir + \"/mdx_extra_q/\" + song_name + \"/drums.wav\"\n",
    "\n",
    "drums_audio, drums_sr = read_audio(drums_only, trim_interval)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ipd.Audio(drums_audio, rate=drums_sr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# No drums\n",
    "no_drums = output_dir + \"/mdx_extra_q/\" + song_name + \"/no_drums.wav\"\n",
    "\n",
    "no_drums_audio, no_drums_sr = read_audio(no_drums, trim_interval)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ipd.Audio(no_drums_audio, rate=no_drums_sr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tapped rhythms version\n",
    "Listen back to tapped rhythms version of the track you picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tapped_path = tapped_dict[song]\n",
    "tapped_audio, tapped_sr = read_audio(tapped_path, trim_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(tapped_audio, rate=tapped_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remixing the tapped rhythms into the original song\n",
    "\n",
    "Now we can replace the original drums with our tapped version of them!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the tapped rhythms are usually lower in volume, so we need to upmix them a bit to make the whole mix clearer sounding\n",
    "mixing_factor = 15\n",
    "tapped_version = no_drums_audio + tapped_audio * mixing_factor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ipd.Audio(tapped_version, rate=tapped_sr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIDI Onset Detection(OD)\n",
    "We can also generate a MIDI file from the tapped rhythms version by using `madmom`'s  [Onset Detection(OD) module](https://madmom.readthedocs.io/en/v0.16/modules/features/onsets.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's calculate the Detection Function(DF) using J. Schlüter and S. Böck's CNN method [1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cnn_od(tapped_audio)\n",
    "df_bins = df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to process this DF and identify which peaks are actually onsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df = peak_picking(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate the onset times from the peaks and we generate a MIDI file from the onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = calculate_onset_times(tapped_audio, tapped_sr, b_df, df_bins)\n",
    "midi_path = od2midi(tapped_path, df, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#midi_path = '../results/demucs/mdx_extra_q/qotsa-no_one_knows/qotsa_no_one_knows-tapped.wav'\n",
    "midi_audio, midi_sr = read_audio(midi_path, trim_interval)\n",
    "print(midi_audio)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(midi_audio, rate=midi_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also remix this OD MIDI version back into our original track:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we also need to upmix the MIDI output\n",
    "mixing_factor = 15\n",
    "pad_width = no_drums_audio.size - midi_audio.size\n",
    "midi_audio_padded = np.pad(midi_audio, (0, pad_width), 'constant')\n",
    "\n",
    "midi_version = no_drums_audio + midi_audio_padded * mixing_factor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " ipd.Audio(midi_version, rate=midi_sr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] [J. Schlüter and S. Böck, ‘Improved musical onset detection with Convolutional Neural Networks’, *2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*, pg. 6979–6983, 2014.](https://ieeexplore.ieee.org/document/6854953)\n",
    "[2] [S. Rouard, F. Massa, and A. Défossez, ‘Hybrid Transformers for Music Source Separation’. *arXiv*, 2022.](https://arxiv.org/abs/2211.08553)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
